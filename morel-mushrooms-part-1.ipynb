{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Good hunting weather: when to look for morel mushrooms\n## Part 1, preparing the data\n\nPrized for their rarity and rich umami flavor, morels inspire more passion than any other edible mushroom. A layer of mystery contributes to the mushroom's allure. Unlike shiitake, lions mane, oysters, or the grocery store button mushrooms, nobody yet has figured out a way to profitably cultivate morels. As such, they remain one of the few food items not available on demand. They must be gathered, not bought.\n\nAnd so every spring, thousands of Americans stalk the forests in search of the mighty morel. The surest bet is to look in places that the mushrooms have been seen before. Elm, ash and poplar trees are commonly associated with morels, as are areas that have been recently burned. \n\nBut when should they go to maximize their chances of success? \n\n### _The question_\n\nWhat weather conditions best predict the occurence of morel mushrooms?\n\n### _The dataset_\n\n<img src=\"morel_sightings_by_month.png\" width=\"900\">\n\nI started with research-grade observations of mushrooms in the Morchella genus uploaded between 2002 - 2022 to the iNaturalist database. iNaturalist is a citizen science project that allows anyone to upload an image of a living thing, which can then be identified by the online community. \"Research Grade\" observations are those which have at least two agreeing IDs, a location, and an exact date/time of observation.\n\nSometimes users upload observations with only a general location. There are technical reasons for this (imprecise GPS readings, lack of GPS connection at time of observation), but in this case there may be subterfuge afoot. Morel spots are jealously guarded, so community naturalists have an added incentive to be vague. Because I'm ultimately going to be associating morel observations with weather variables, I chose to include only those for which latitude-longtitude coordinates were accurate to with 1000 meters. \n\nGBIF.org (14 December 2022) GBIF Occurrence Download https://doi.org/10.15468/dl.fudjcr","metadata":{}},{"cell_type":"code","source":"# Import libraries to preview the data\nimport pandas as pd","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(\"morchella_observations_cleaned.csv\")\ndf.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There's some metadata about each observation that's not very relevant to my project. I'll tighten up the dataframe to include only what I'm interested in: location, time of observation, and (possibly) the species of morel. Let's see what can be dropped.","metadata":{}},{"cell_type":"code","source":"df.columns","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = df.drop(columns=['gbifID','datasetKey','occurrenceID','Unnamed: 12'])\ndf.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Better! I'm not sure if I'm going to make a species-level analysis. It would help to know how many observations there are for each species","metadata":{}},{"cell_type":"code","source":"df.species.value_counts()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Most of the observations are of Morchella americana. Of the other species observed, there are six with large enough counts to potentially draw some useful conclusions about occurence. I'll start with all the morel observations together, then perhaps do a species by species analysis.\n\nI need to find the weather in the week preceding each observation. Based on what is already known about morel fruiting habits, I'm interested in these variables:\n\n* Soil temperature\n* Air temperature\n* Amount of precipitation\n* Relative humidity\n\nTo do this, I'll need to use an API with historical weather data. I'm grateful to find that Open Meteo offers just such a resource. [See here](https://openmeteo.substack.com/p/60-years-of-historical-weather-as) for more on the heruculean process of creating it.\n\nAPI calls take this form: https://archive-api.open-meteo.com/v1/era5?latitude=52.5201&longitude=13.4121&start_date=2022-11-10&end_date=2022-12-09&temperature_unit=fahrenheit&hourly=temperature_2m,relativehumidity_2m&daily=temperature_2m_max,temperature_2m_min,precipitation_sum and return a JSON object.\n\nNote that the API supports coordinate precision to four points and that dates are in the \"YYYY-MM-DD\" format. To minimize the amount of format-wrangling in my API call functions, I'll edit the the dataframe that meet these requirements.","metadata":{}},{"cell_type":"code","source":"api_df = df.round({'decimalLatitude': 4, 'decimalLongitude': 4})\napi_df['date'] = api_df.agg('{0[year]}-{0[month]:02d}-{0[day]:02d}'.format, axis=1)\napi_df.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Import libraries to work with API data\nimport requests\nimport json\nimport datetime\n\napi_url = 'https://archive-api.open-meteo.com/v1/era5'\nweather_data_request = \"hourly=temperature_2m,relativehumidity_2m,soil_temperature_0_to_7cm&daily=precipitation_sum&timezone=auto\"\n\nweather_data = []\n\nfor row in api_df.iterrows():\n    row = row[1]\n    latitude, longitude = row['decimalLatitude'], row['decimalLongitude']\n    end_date = datetime.date(row['year'],row['month'],row['day'])\n    start_date = end_date - datetime.timedelta(days=3)\n\n    end_date = end_date.strftime(\"%Y-%m-%d\")\n    start_date = start_date.strftime(\"%Y-%m-%d\")\n\n    base_url = f\"{api_url}?latitude={latitude}&longitude={longitude}&start_date={start_date}&end_date={end_date}&\"\n    request_url = base_url+weather_data_request\n\n    response = requests.get(request_url)\n    weather_data.append(response.json())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"weather_df=pd.DataFrame.from_records(weather_data)\n\ncombined_df=pd.concat([df,weather_df], axis=1)\ncombined_df.head()\n\ncombined_df.to_csv('morchella_observations_and_weather.csv')","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I am absolutely certain that there are smoother and more canonical ways to do this, both in pandas and in python, but now I have a dataset that links each observation with the weather from the four days that preceded it. If I were dealing with a larger dataset, I would break this out into a database, but since this consitutes only about 1800 observations, I'm going to continue work from a CSV file.\n\nAt this stage, there are already some things I'd like to have done better:\n\n* Generate the API request links without iterating through the dataframe. When dealing with pandas data, there's usually a better approach than iterating\n* Request from the API in a faster way. Execution of that block took 17 minutes! It might have gone faster had I used Python's asyncio library to iterate and make API requests concurrently.\n\nIn the next notebook, I'll explore the data to make connections between weather and morel appearances. I'm envisioning a few challenges:\n\n* Each observation is now linked to a time series with both hourly and daily weather information. Basically, each row of the dataframe got a lot wider or longer. I've never dealt with data in this format. I think R will be better suited to the need to nest dataframes within eachother.\n* The time series data is stored in a bunch of json objects. I'm not sure the best way to parse them. ","metadata":{}}]}